# Homework 2

> Author: Nicholas M. Synovic

## Table of Contents

- [Homework 2](#homework-2)
  - [Table of Contents](#table-of-contents)
  - [About](#about)
  - [Dependencies](#dependencies)
  - [How To Run](#how-to-run)
  - [Methodology](#methodology)
    - [Data Splitting](#data-splitting)
    - [Naive Bayes Implementation](#naive-bayes-implementation)
    - [Generating Vocabularly](#generating-vocabularly)
    - [Computing Word Frequency](#computing-word-frequency)
    - [Computing Class Likelihoods per Word](#computing-class-likelihoods-per-word)
    - [Testing Naive Bayes](#testing-naive-bayes)
  - [Results](#results)

## About

The homework assignment description can be found in [hw2.pdf](hw2.pdf). The
[`hw2.py`](hw2.py) script is the executable code to run to generate results.

The dataset used for this assignment was downloaded from
[here](https://github.com/dennybritz/cnn-text-classification-tf/tree/master/data/rt-polaritydata).

## Dependencies

To run this code, you will need:

- `Python 3.10`
- `requests`
  - This can be installed by running `pip install -r requirements.txt`

## How To Run

- `python3.10 hw2.py`

## Methodology

My methodology follows the algorithm description from
[Figure 4.2 of the text](https://web.stanford.edu/~jurafsky/slp3/4.pdf#figure.4.2),
I first computed the overall document frequency for each class.

### Data Splitting

The dataset was split with the following method:

1. Load in the positive dataset
1. Use *the first 70%* of indexes to create **the positive training dataset**
1. Use *the next 15%* of indexes to create **the positive development dataset**
1. Use *the remaining 15%* of index to create **the positive testing dataset**
1. Repeat steps 1 - 4 and substitute *the positive dataset* for *the negative
   dataset* (thus resulting *in the same splits percentages* for **the negative
   training, development, and testing datasets**)

The results of the splitting are the following:

```text
Positive Training Doc. Size     : 3732   (69.9925% of Positive Docs)
Positive Development Doc. Size  : 800    (15.00375% of Positive Docs)
Positive Testing Doc. Size      : 799    (14.985% of Positive Docs)

Negative Training Doc. Size     : 3732   (69.9925% of Negative Docs)
Negative Development Doc. Size  : 800    (15.00375% of Negative Docs)
Negative Testing Doc. Size      : 799    (14.985% of Negative Docs)
```

### Naive Bayes Implementation

> **NOTE**: Any `log` operations where done using Python's `math.log10` function
>
> **NOTE**: This algorithm was first tested on the development datasets. Results
> for these tests can be found in [Results](#results)

Following the algorithm description from
[Figure 4.2 of the text](https://web.stanford.edu/~jurafsky/slp3/4.pdf#figure.4.2),
I first computed the overall document frequency for each class.

This was done by performing the following algorithm:

```python
data: List[str] = positiveData + negativeData

positiveClassLog = abs(log10(len(positiveData) / len(data)))
negativeClassLog = abs(log10(len(negativeData) / len(data)))
```

Where `positiveData` and `negativeData` are the **positive dataset** and
**negative dataset** calculated from during the
[data splitting operation](#data-splitting) respectfully.

### Generating Vocabularly

The vocabulary was generated by first joining the positive and negative
documents from the training datasets. And then splitting the documents into
words by splitting on the spaces in between each word.

### Computing Word Frequency

Word frequency is the count of a word within a set of documents in a class. To
compute this, I used a `defaultdict(int)` to store the mapping of
`{word : count}`.

I first split each document in a class by spaces and joined all the resulting
lists of words. I then iterated over each word, appended it to the the list, and
set the value for each word to be `1+` the previous value.

### Computing Class Likelihoods per Word

The class likelihood per word is the measurement of how likely a word is to
appear within a set of documents given a class. It is measured as a percentage.

To compute this, I took the word counts for a given class from
[Computing Word Frequency](#computing-word-frequency) and divded is by total
number of words within said class. Laplace smoothing of 1 was applied to both
the total number of words as well as the word count. The `log10()` was taken of
the quotient.

This operation was done twice; once for the positive class and once for the
negative class.

The two quotients were then appended to a list and stored in a dictionary. An
example dictionary is described below:

```python
classLikelihood: dict[str, List[float, float]] = {"word": [0.1, 0.2]}
```

### Testing Naive Bayes

Combining the above functions together results in a functional Naive Bayes
algorithm.

Tests were done using the development dataset while creating this algorithm. The
training and development dataset were concatinated together prior to evaluating
the testing dataset.

To determine the class probability of a document, each token of the document was
looked up in the
[class likelihood mapping](#computing-class-likelihoods-per-word) and summed
together. Both the positive and negative class likelihoods were consoluted to
determine the positive and negative classes respectfully.

If a word in the testing dataset did not exist within the class likelihood
mapping, then no value for that word was added (equivalent to a `+ 0`
operation).

If the positive class likelihood was greater than the negative class likelihood,
then it was reported that it was a positive class. Else, it was reported as a
negative class.

## Results

The following table includes the accuracy results from my Naive Bayes
implementation.

|                         | **True Positive** | **True Negative** | **False Positive** | **False Negative** |
| ----------------------- | ----------------- | ----------------- | ------------------ | ------------------ |
| **Development Dataset** | 73.0%             | 72.75%            | 27.0%              | 27.25%             |
| **Testing Dataset**     | 72.59074%         | 77.09637%         | 27.40926%          | 22.90363%          |
