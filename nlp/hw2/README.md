# Homework 2

> Author: Nicholas M. Synovic

## Table of Contents

- [Homework 2](#homework-2)
  - [Table of Contents](#table-of-contents)
  - [About](#about)
  - [Dependencies](#dependencies)
  - [How To Run](#how-to-run)
  - [Methodology](#methodology)
    - [Data Splitting](#data-splitting)
    - [Naive Bayes Implementation](#naive-bayes-implementation)
    - [Generating Vocabularly](#generating-vocabularly)
    - [Computing Word Frequency](#computing-word-frequency)
  - [Results](#results)

## About

The homework assignment description can be found in [hw2.pdf](hw2.pdf). The
[`hw2.py`](hw2.py) script is the executable code to run to generate results.

The dataset used for this assignment was downloaded from
[here](https://github.com/dennybritz/cnn-text-classification-tf/tree/master/data/rt-polaritydata).

## Dependencies

To run this code, you will need:

- `Python 3.10`
- `requests`
  - This can be installed by running `pip install -r requirements.txt`

## How To Run

- `python3.10 hw2.py`

## Methodology

My methodology follows the algorithm description from
[Figure 4.2 of the text](https://web.stanford.edu/~jurafsky/slp3/4.pdf#figure.4.2),
I first computed the overall document frequency for each class.

### Data Splitting

The dataset was split with the following method:

1. Load in the positive dataset
1. Use *the first 70%* of indexes to create **the positive training dataset**
1. Use *the next 15%* of indexes to create **the positive development dataset**
1. Use *the remaining 15%* of index to create **the positive testing dataset**
1. Repeat steps 1 - 4 and substitute *the positive dataset* for *the negative
   dataset* (thus resulting *in the same splits percentages* for **the negative
   training, development, and testing datasets**)

The results of the splitting are the following:

```text
Positive Training Data Size     : 3731 (69.98687% of Positive Data)
Positive Development Data Size  : 800 (15.00657% of Positive Data)
Positive Testing Data Size      : 799 (14.98781% of Positive Data)

Negative Training Data Size     : 3730 (69.99437% of Negative Data)
Negative Development Data Size  : 799 (14.99343% of Negative Data)
Negative Testing Data Size      : 799 (14.99343% of Negative Data)
```

### Naive Bayes Implementation

> **NOTE**: Any `log` operations where done using Python's `math.log10` function
>
> **NOTE**: This algorithm was first tested on the development datasets. Results
> for these tests can be found in [Results](#results)

Following the algorithm description from
[Figure 4.2 of the text](https://web.stanford.edu/~jurafsky/slp3/4.pdf#figure.4.2),
I first computed the overall document frequency for each class.

This was done by performing the following algorithm:

```python
data: List[str] = positiveData + negativeData

positiveClassLog = abs(log10(len(positiveData) / len(data)))
negativeClassLog = abs(log10(len(negativeData) / len(data)))
```

Where `positiveData` and `negativeData` are the **positive dataset** and
**negative dataset** calculated from during the
[data splitting operation](#data-splitting) respectfully.

### Generating Vocabularly

The vocabulary was generated by first joining the positive and negative
documents from the training datasets. And then splitting the documents into
words by splitting on the spaces in between each word.

### Computing Word Frequency

## Results
